\documentclass[11pt]{article}

%  USE PACKAGES  ---------------------- 
\usepackage[margin=0.7in,vmargin=1in]{geometry}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref,color}
\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{4}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\HREF}[2]{\href{#1}{#2}}
\usepackage{textcomp}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
% columns=flexible,
upquote=true,
breaklines=true,
showstringspaces=false
}
%  -------------------------------------------- 

%  HEADER AND FOOTER (DO NOT EDIT) ----------------------
\newcommand{\problemnumber}{0}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{\textbf{Question \problemnumber}}
\newcommand{\newquestion}[1]{
\clearpage % page break and flush floats
\renewcommand{\problemnumber}{#1} % set problem number for header
\phantom{}  % Put something on the page so it shows
}
\fancyfoot[L]{IE 332}
\fancyfoot[C]{Assignment 2}
\fancyfoot[R]{Page \thepage}
\renewcommand{\footrulewidth}{0.4pt}

%  --------------------------------------------


%  COVER SHEET (FILL IN THE TABLE AS INSTRUCTED IN THE ASSIGNMENT) ----------------------
\newcommand{\addcoversheet}{
\clearpage
\thispagestyle{empty}
\vspace*{0.5in}

\begin{center}
\Huge{{\bf IE332 Assignment \#2}} % <-- replace with correct assignment #

Due: April 14th, 11:59pm EST % <-- replace with correct due date and time
\end{center}

\vspace{0.3in}

\noindent We have {\bf read and understood the assignment instructions}. We certify that the submitted work does not violate any academic misconduct rules, and that it is solely our own work. By listing our names below we acknowledge that any misconduct will result in appropriate consequences. 

\vspace{0.2in}

\noindent {\em ``As a Boilermaker pursuing academic excellence, I pledge to be honest and true in all that I do.
Accountable together -- we are Purdue.''}

\vspace{0.3in}

\begin{table}[h!]
  \begin{center}
    \label{tab:table1}
    \begin{tabular}{c|ccccc|c|c}
      Student & Q1 & Q2 & Q3 & Q4 & Q5 & Overall & DIFF\\
      \hline
      Talya Arpaci & 10 & 60 & 10 & 10 & 10 & 100 & 0\\
      Hershi Meher & 30 & 10 & 50 & 10 & 10 & 110 & +10\\
      Mohamed Abuelreish & 30 & 10 & 10 & 10 & 10 & 70 & -30\\
      Andrew Newquist & 10 & 10 & 10 & 60 & 60 & 150 & +50\\
      Nicholas Alfano & 10 & 10 & 20 & 10 & 10 & 60 & -40\\
      \hline
      St Dev & 10.95 & 22.36 & 17.32 & 22.36 & 22.36 & 37.01 & 35.64
    \end{tabular}
  \end{center}
\end{table}

\vspace{0.2in}

\noindent Date: \today.
}
%  -----------------------------------------

%  TODO LIST (COMPLETE THE FULL CHECKLIST - USE AS EXAMPLE THE FIRST CHECKED BOXES!) ----------------------
\newcommand{\addtodo}{
\clearpage
\thispagestyle{empty}

\section*{Read Carefully. Important!}

\noindent By electronically uploading this assignment to Brightspace you acknowledge these statements and accept any repercussions if in any violation of ANY Purdue Academic Misconduct policies. You must upload your homework on time for it to be graded. No late assignments will be accepted. {\bf Only the last uploaded version of your assignment before the due date will be graded}.

\vspace{0.2in}

\noindent {\bf NOTE:} You should aim to submit no later than 30 minutes before the deadline, as there could be last minute network traffic that would cause your assignment to be late, resulting in a grade of zero. 

\vspace{0.2in}

\noindent When submitting your assignment it is assumed that every student considers the below checklist, as there are grading consequences otherwise (e.g., not submitting a cover sheet is an automatic grade of ZERO).

\begin{todolist}

    \item[\done] Your solutions were prepared using the \LaTeX template provided in Brightspace. 
    \item[\done] Your submission has a cover sheet as its first page and this checklist as its second page, according to the template provided.
	 \item[\done] All of your solutions (program code, etc.) are included in the submission as requested. % Check this checkbox and the following ones if satisfied <---
    \item[\done] You have not included any screen shots, photos, etc. (plots should be intermediately saved as .png files and then added into your .tex file). % <---
	 \item[\done] All math notation and algorithms (algorithmic environment) are created using appropriate \LaTeX code (no pictures, handwritten solutions, etc.). % <---
    \item[\done] The .pdf is submitted as an individual file and not in a {\tt .zip}.
    \item[\done] You kept the \LaTeX source code in your files until this assignment is graded, in case you are required to show proof of creating your assignment using \LaTeX.  % <---
    \item[\done] If submitting with a partner, your partner is added in the submission section in Gradescope after you upload your file. % <---
    \item[\done] You have correctly matched each question to its page \# in the .pdf submission in the Gradescope section (after you uploaded your file).
    \item[\done] Watch videos on creating pseudocode if you need a refresher or quick reference to the idea. These are good starter videos:    % <---
    
     \HREF{https://www.youtube.com/watch?v=4jLO0vXPktU}{www.youtube.com/watch?v=4jLO0vXPktU} 
    
    \HREF{https://www.youtube.com/watch?v=yGvfltxHKUU}{www.youtube.com/watch?v=yGvfltxHKUU}
\end{todolist}
}

%% LaTeX
% Für alle, die die Schönheit von Wissenschaft anderen zeigen wollen
% For anyone who wants to show the beauty of science to others

%  -----------------------------------------


\begin{document}


\addcoversheet

\addtodo

% BEGIN YOUR ASSIGNMENT HERE:
\newpage

\fancyhead[L]{\textbf{Question 1}}
\section{Question 1}
% Question 1

% \noindent Example of proper typesetting of quotation marks:\\
% ``proper opening and closing quotation marks'' vs "bad quotation marks".
\subsection{Part A}
Part A Written Explanation:
The outer loop in lines 2-8 iterates n times. For each iteration, the inner loop in lines 4-7 iterates n/i times, where i is the loop counter in the outer loop.  Therefore, the total number of iterations of the inner loop can be expressed as: \begin{math}\Sigma(n/i) for i = 1 to n\end{math}
This is a harmonic series, which has the following asymptotic behavior:
\begin{math}\Sigma(n/i) for i = 1 to n = \Theta(n log n)\end{math}
Since the time complexity of the function g(j) in line 5 is \begin{math}\Theta(1)\end{math}, the total time complexity of the algorithm can be expressed as: \begin{math}\Theta(n log n)\end{math}
The time complexity of this algorithm: \begin{math}\Theta(n log n)\end{math}.



\subsection{Part B}
Part B Written Explanation:
The given equation \begin{math}\sqrt{n}*n=(n^{3/2})\end{math} can be simplified to show that the total number of iterations of the inner loop is equal to \begin{math}n^({3/2})\end{math}. Each iteration of the inner loop calls function h(j), which has a time complexity of \begin{math}\Theta(1)\end{math}. Therefore, the time complexity of the algorithm can be calculated by multiplying the number of iterations of the inner loop with the time complexity of function h(j), which results in a time complexity of \begin{math}\Theta(n^{3/2})\end{math}. It is important to note that the \begin{math}\Theta\end{math} notation indicates that the algorithm's time complexity is bounded both from above and below by a constant multiple of \begin{math}n^{3/2}\end{math}, which means that the algorithm's running time grows at the same rate as \begin{math}n^{3/2}\end{math}, up to a constant factor. In summary, the given equation, along with the complexities of the inner loop and function h(j), leads to a time complexity of \begin{math}\Theta = \Theta(n^{3/2})\end{math} for the algorithm.

\subsection{Part C}
Part C Written Explanation:
The outer loop iterates from i=3 to n, so it runs (n-3+1) = (n-2) times.
The inner loop iterates while j $\le$ n/i, which means that it runs (n/i + 1) times. Assuming integer division here: n/i = floor(n)/i.
Therefore, the total number of iterations of the inner loop over all iterations of the outer loop is: \begin{math}\Sigma\end{math}(n/i + 1), where i goes from 3 to n.

Simplifying:
\begin{math}\Sigma\end{math}(n/i + 1) = \begin{math}\Sigma\end{math}(n/i) + \begin{math}\Sigma\end{math}(1)
= n\begin{math}\Sigma\end{math}(1/i) + (n-2)
= n(1 + 1/2 + 1/3 + ... + 1/n) + (n-2)

The sum (1 + 1/2 + 1/3 + ... + 1/n) is the harmonic series, which has time complexity log(n). Therefore, we can simplify the total number of iterations as:
n(log(n) + 1) + (n-2) = nlog(n) + n - 2
Since each iteration of the inner loop takes constant time (line 5 and line 6 have time complexity of 1), the total running time of the algorithm is \begin{math}\Theta\end{math}(nlog(n)).
Time complexity of this algorithm: \begin{math}\Theta\end{math}(nlog(n)).


\subsection{Part D}
Part D Written Explanation:
The time complexity of this algorithm depends on the value of the input number x. The algorithm described operates using a while loop, which will run until a condition is met. Specifically, the loop will run until x is less than or equal to 1, which is the worst-case scenario. During each iteration, x will be manipulated in a variety of ways depending on its parity and divisibility by 3. If x is odd, it will be multiplied by 2 in each iteration until it becomes even. This will occur a total of O(logx) times. If x is even and divisible by 3 but not 2, it will also be multiplied by 2 in each iteration until it becomes even, which will also take O(logx) iterations. If x is even and not divisible by 3, it will be divided by 2 using a function g(x) in each iteration until it becomes odd. This operation will also take O(logx) iterations. In total, the while loop will execute O(logx) times. The function g(x) will take O(logx) time as well. Thus, the time complexity of the algorithm is \begin{math}O(log^{2}x)\end{math}, which is denoted by \begin{math}\Theta = O(log^{2}x)\end{math}.


\subsection{Part E}
Part E Written Explanation:
e)	The given algorithm contains a while loop that iterates over the elements of the input list x (n times).
For each element, there is a conditional statement that either appends a new element or removes an existing one from the list. 
The conditional statement is followed by a call to a function g(x), which operates on list x.
The function g(x) is called in both cases, and its time complexity depends on the size of the input list. According to the given information, g(x) takes \begin{math}n^{2}\end{math} time when the even-indexed element is being processed, and log(n) time when the odd-indexed element is being processed.
Therefore, to analyze the time complexity of this algorithm, we need to consider two cases: when the even-indexed elements are being processed and when the odd-indexed elements are being processed.

Case 1: Even-indexed elements
For even-indexed elements, the while loop iterates n/2 times. Within each iteration, there are two constant-time operations (line 4 and 5) followed by a call to g(x), which takes \begin{math}n^{2} \end{math} time. Therefore, the time complexity of the loop for even-indexed elements is:

\begin{math}(n/2) * (2 + n^{2}) = (n^{3/2}) + n\end{math}

Case 2: Odd-indexed elements
For odd-indexed elements, the while loop iterates n/2 times. Within each iteration, there are two constant-time operations (line 7 and 8) followed by a call to g(x), which takes log(n) time. Therefore, the time complexity of the loop for odd-indexed elements is:

\begin{math}(n/2) * (2 + log(n)) = (nlog(n)/2) + n\end{math}

The overall time complexity of the algorithm is the sum of the time complexities of the two cases = \begin{math}\Theta((n^{3/2}) + (nlog(n)/2) + n)\end{math}.
This can be simplified to \begin{math}\Theta(n^{3})\end{math}, since \begin{math}n^3\end{math} is the dominant term in the expression. 
Simplified time complexity of this algorithm: \begin{math}\Theta(n^{3}).\end{math}



\subsection{Part F}

\underline{Algorithm 1}:
What the algorithm is supposed to do: 
The algorithm is supposed to sort the input list x in ascending order using a selection sort technique that iteratively selects the smallest element from the unsorted portion of the array and moves it to the beginning of the sorted part of the array.
Analysis of run-time complexity:
The outer loop iterates n times (line 3), where n is the length of the input list x.
The inner loop calls the function "findmin" on a sub-list of x (line 4), which has a length that decreases by 1 at each iteration of the outer loop.
The "findmin" function needs to iterate over the sub-list to find the minimum value and its index. This operation has a time complexity of O(n).
Deleting an element from the list (line 5) takes O(n) time in the worst case because it requires shifting all the elements that come after the deleted element.
Pushing an element to the list (line 6) takes O(1) time in the best case and O(n) time in the worst case if the list needs to be resized.
Therefore, the worst-case run-time complexity of the algorithm is \begin{math}O(n^{2})\end{math}, which occurs when the input list x is sorted in descending order, and each deletion and insertion operation takes O(n) time.

T function for the worst case:
\begin{math}T(n) = n * (n + 1) / 2 * O(n) = O(n^{2})\end{math}

Algorithm for line 5 with multiple operations (delete an element from a list):
\begin{lstlisting}[language=R, frame=single]
1. Input: list x, index i of the element to delete
2. For j = i to length(x) - 1 do
3.     x[j] = x[j+1]
4. End For
5. Remove the last element of x
\end{lstlisting}

Run- time complexity of line 5: O(n), where n is the length of the list x.

Algorithm for line 6 with multiple operations (push an element to the end of a list):
\begin{lstlisting}[language=R, frame=single]
1. Input: list x, value v to push
2. If length(x) < capacity(x) then
3.     x[length(x)+1] = v
4. Else
5.     Allocate a new array y with larger capacity
6.     For i = 1 to length(x) do
7.         y[i] = x[i]
8.     End For
9.     y[length(x)+1] = v
10.    x = y
11. End If
\end{lstlisting}
Worst-case run-time complexity of line 6: O(n), where n is the length of the list x, because it requires copying all the elements of x to a new array when the capacity of x is exceeded. However, in the best case when there is enough capacity in the array, the best-case run-time complexity of line 6: O(1).

\underline{Algorithm 2}:
What the algorithm is supposed to do: 
The given algorithm implements the Insertion Sort algorithm, which sorts a given list of numeric elements in ascending order. The algorithm iterates over the list, and for each element, it compares it to the elements on its left side and moves it to the correct position. This is done by swapping the current element with the element on its left until the current element is in its correct position.

Analysis of run-time complexity:
The run-time complexity of the inner loop (Steps 5-9) is O(i) in the worst case, where i is the number of elements that need to be shifted to insert the current element in its correct position. Since i can be at most n-1, the worst-case run-time complexity of the inner loop is O(n). 
The outer loop (Steps 3-11) iterates n times, and for each iteration, the inner loop can iterate up to i times. 
Hence, the worst-case run-time complexity of the algorithm: \begin{math}O(n^{2})\end{math}.

T function for the worst-case is:
\begin{math}T(n) = O(n^{2})\end{math}

\underline{Algorithm 3}:
What the algorithm is supposed to do: 
The given algorithm is an implementation of the Bubble Sort algorithm, which is used to sort a list of numeric elements in ascending order. It repeatedly compares adjacent elements in the list and swaps them if they are in the wrong order. The algorithm continues this process until no more swaps are required, indicating that the list is sorted.

Analysis of run-time complexity:
Step 2: Assigning the length of the list to the variable n takes O(1) time.
Step 3: Assigning the value true to the variable swapped takes O(1) time.
Step 4: The while loop runs until swapped is false, which can take up to n iterations. Each iteration of the while loop takes O(n) time, since it contains a nested for loop.
Steps 6-13: The for loop iterates over each element in the list, performing a comparison and a possible swap for each element. This loop takes O(n) time for each iteration of the while loop, since it runs n-1 times.
Steps 7-12: The if statement checks if the current element is greater than its adjacent element and performs a swap if necessary. These operations take O(1) time each.

In the worst case, the algorithm has to perform n-1 comparisons for each of the n elements in the list, resulting in a total of (n-1) * n/2 comparisons. Thus, the worst-case run-time complexity of the algorithm: \begin{math}O(n^{2})\end{math}

T function for the worst case is:
 \begin{math}T(n) = O(n^{2}).\end{math}



\subsection{Part G}

The time functions for each of the 3 algorithms can be plotted by defining the x-axis to be the length of the input array x = n, and the y-axis to be the worst-case time complexity functions for the respective sorting algorithms.
Below is the Python code to plot the T functions for 1) Selection Sort, 2) Insertion Sort, and 3) Bubble Sort algorithms:
\begin{lstlisting}[language=Python, frame=single]
#Important: Use most recent version of Python to make sure the modules load
import time
from random import randint
import matplotlib.pyplot as plt 
from algorithms.sort import selection_sort, insertion_sort, bubble_sort

#1)Worst-case time complexity for Selection Sort
list1 = [i for i in range(5000)]
times_selection=[]
for x_1 in range(0,1000,10):
    start_time = time.time()
    list2 = selection_sort(list1[:x_1])
    elapsed_time = time.time() - start_time
    times_selection.append(elapsed_time)
print(times_selection)
x_1=[i for i in range(0,1000,10)]

#matplotlib inline
plt.xlabel("Length of input array x")
plt.ylabel("Time function for Selection Sort")
plt.plot(x_1,times_selection)

#2) Worst-case time complexity for Insertion Sort
list3 = [i for i in range(5000)]
times_insertion=[]
for x_2 in range(0,1000,10):
    start_time = time.time()
    list4 = insertion_sort(list3[:x_2])
    elapsed_time = time.time() - start_time
    times_insertion.append(elapsed_time)
print(times_insertion)
x_2=[i for i in range(0,1000,10)]

#matplotlib inline
plt.xlabel("Length of input array x")
plt.ylabel("Time function for Insertion Sort")
plt.plot(x_2,times_insertion)

#3) Worst-case time complexity for Bubble-Sort
from algorithms.sort import bubble_sort
list5 = [i for i in range(5000)]
times_bubble=[]
for x_3 in range(0,1000,10):
    start_time = time.time()
    list6 = bubble_sort(list5[:x_3])
    elapsed_time = time.time() - start_time
    times_bubble.append(elapsed_time)
print(times_bubble)
x_3=[i for i in range(0,1000,10)]

#matplotlib inline
plt.xlabel("Length of input array x")
plt.ylabel("Time function for Bubble Sort")
plt.plot(x_3,times_selection)
\end{lstlisting}

For the worst-case time complexities of each sorting algorithm, the T functions do not intersect.
For smaller sizes of the input array x (i.e., smaller n), there is a slight chance that the T functions could intersect. As can be seen from the plots generated by the code above though, as the size of the input array x increases, the difference between the T functions of the three sorting algorithms grows and the chance of intersection becomes negligible.
Insertion Sort performs better than Bubble Sort and Selection Sort for small arrays, as it requires fewer comparisons and swaps. ‘Thus, for smaller arrays, Insertion Sort is the best option.

When we have large arrays as input (i.e., large n), the selection algorithm with the smallest constant factor in its T function would minimize the overall run-time of the algorithm.
Bubble Sort is not efficient for large arrays. While Selection Sort performs better than Bubble Sort at sorting large arrays, it is still not the best option for very large arrays. While Insertion Sort is efficient for small arrays, as stated before, it may not be the best option for very large arrays. 
In conclusion, Bubble Sort, Selection Sort, and Insertion Sort have worst-case time complexities of \begin{math}O(n^{2})\end{math}, which means that their efficiency decreases with increasing input array sizes. However, actual performance may depend on the quality of the input array- how far from sorted it is. Thus, while it is possible to determine the best algorithm out of the three (Insertion Sort) for smaller input arrays simply by analyzing the algorithm steps, for larger input arrays it might be best to consider algorithms other than these three- like Quick Sort or Merge Sort. 


\newpage

% Question 2
\fancyhead[L]{\textbf{Question 2}}
\section{Question 2}
\subsection{Part A}

Part A Written Explanation:
The loop invariants are count and i that seem to increment once loop and if statements are held true, which makes the code run (as long as i is less than the amount of attempts to run the code). These  conditions holds throughout the code. The loop invariant mostly seems correct but we would like to place i++ after the ``end if" as we want count to be the number of points fallen within the circle and i to be the number of total points.
\begin{lstlisting}[language=R, frame=single]
Require: x, as a number of attempts
1: Function f(x)
2: count=0
3: i=0
4: while i < x do
5: xc=runif(-1,1)
6: yc=runif(-1,1)
7: if xc2 + yc2 < 1 then
8: count++
9: end if
10: i++
11: end while
println(count/i*4)
12: EndFunction
\end{lstlisting}

\subsection{Part B}
Part B R Code:
\begin{lstlisting}[language=R, frame=single]
library(purrr)

f <- function(x) {
  count <- 0 #initialize count
  i <- 0 #initialize i
  while (i < x) { #begin while loop
    xcyc <- map_dbl(rep(runif(1,-1,1), 2), `^`, 2) #create a vector of x^2 and y^2
    if (sum(xcyc) < 1) { # if sum of x^2+ y^2<1
      count <- count + 1 #increment count
    }
    i <- i + 1 #increment i
  }
  print(count/i*4) #print result
}
\end{lstlisting}

\newpage
\subsection{Part C}
Part C R Code:
\begin{lstlisting}[language=R, frame=single]
library(purrr)
f <- function(x, count=0, i=0){  # intialize a function that passes x, count=0, i=1 
  xcyc =map_dbl(rep(runif(1,-1,1), 2), `^`, 2) #create a vector of x^2 and y^2
  if (sum(xcyc)<1) { #if sum of x^2+ y^2<1
  count<-count+1  #increment count
  }
  i<-i+1 #increment i
  if (i<x) # introduce gurad statement
  return(f(x, count, i)) #if statement holds return to the function and bring same x, new count and new i
  else{
    print(count/i*4) #print result
  }
}
\end{lstlisting}


Part C Written Explanation:

In the original case, $i<x$ is a while loop condition until the code has run out of attempts.
In this case $i<x$ is an if statement, serving as a guard expression. Recursivity is put into an if statement with an if statement $i<x$ that returns to the initial function by passing x, count, and i variables to its next iteration within the function. With every recursive run i and count get larger and once i it gets equal to x, it stops the function and returns the final result of the code. \newline

\newline Let us try to prove the correctness of this code.\newline
\newline Initialization: Once we enter the code with x attempts, count =0, and i=0  the code will generate a random point within -1,1 and square them. At this stage is i<x. Then if the sum of the coordinates is less then 1 the count increases by 1 with an afterward increase of i. \newline
\newline Maintenance: To see that each iteration maintains $i<x$, observe how i and count  change and compare it to x. Once the condition $i<x$ holds, we return to the function with the same value of x and the new increased count and i. The next iteration is entered with x, count=1 and i=1.\newline
\newline Termination: Eventually, having this process repeated, i will become equal to x, and the condition ($i<x$) terminates, thus count and i stop incrementation, leading to the else statement return(count/i*4). Which means that invariant $i<x$ still holds.

\newpage
% Question 3
\fancyhead[L]{\textbf{Question 3}}
\section{Question 3}
\subsection{Initializing Workspace}
R Code:
\begin{lstlisting}[language=R, frame=single]
#Important:
#All non-code answers are included in the code as comments
#The data has been loaded using the file path that is specific to my (Hershi's) computer. Please do change it when loading the dataset.
#Some sections of the code, especially the plots, take a longer time to load. Please consider the fact that the dataset is huge when encountering this delay.


#Import the data and clean it of Na's. Convert neg,pos to 0,1.

install.packages("dplyr", repos = "http://cran.us.r-project.org")
library(dplyr)
aps_failure_data_clean <- read.csv("C:\\Users\\surre\\Desktop\\IE 332\\A2 Q3\\aps_failure_training_set.csv", skip = 20, na.strings = "na")
aps_failure_data_clean$class <- ifelse(aps_failure_data_clean$class == "pos", 1, 0)

#Setting the dimension of the data set to 20000
aps_failure_data_clean <- head(aps_failure_data_clean, 20000)

#splitting the clean data set into train/test sets using 70% for training and 20% for testing
create_train_test <- function(aps_failure_data_clean, size = 0.7, train = TRUE) {
  n_row = nrow(aps_failure_data_clean)
  total_row = size * n_row
  train_sample = 1: total_row
  if (train == TRUE) {
    return (aps_failure_data_clean[train_sample, ])
  } else {
    return (aps_failure_data_clean[-train_sample, ])
  }
}
data_train1 <- create_train_test(aps_failure_data_clean, 0.7, train = TRUE)
data_test1 <- create_train_test(aps_failure_data_clean, 0.7, train = FALSE)

\end{lstlisting}
\newpage
\subsection{Part A}
Part A R Code:
\begin{lstlisting}[language=R, frame=single]
#Installing the necessary packages
install.packages("rpart", repos="http://cran.us.r-project.org")
install.packages("rpart.plot", repos="http://cran.us.r-project.org")
install.packages("caret", repos="http://cran.us.r-project.org")
install.packages("parsnip", repos="http://cran.us.r-project.org")

# Loading the necessary packages
library(rpart.plot)
library(rpart)
library(caret)
library(parsnip)

#Training the decision tree using training data
fit1 <- rpart(class ~ ., data = data_train1, maxdepth = 20)

#Plotting decision tree
rpart.plot(fit1, type = 2)

#Applying to the testing data
predict_test1 <- predict(fit1, data_test1, type = "vector")

#Calculating testing risk
risk_test1 <- mean(predict_test1 != data_test1$class)

#Applying to training data and calculating training risk
predict.train1 <- predict(fit1, data_train1, type = "vector")
risk_train1 <- mean(predict.train1 != data_train1$class)

train_false_neg_vector <- vector("numeric", 20)
test_false_neg_vector <- vector("numeric", 20)

for (i in 1:20) {
  # Training a decision tree model using the training data  
  fit1 <- rpart(class ~ ., data = data_train1, cp = 0, maxdepth = i + 1)
  predict.train1 <- predict(fit1, data_train1, type = "vector")
  predict_test1 <- predict(fit1, data_test1, type = "vector")
  
  # Calculating the false negative rates for training and testing data sets
  train_confusion_matrix1 <- table(data_train1$class, predict.train1)
  test_confusion_matrix1 <- table(data_test1$class, predict_test1)
  
  train_false_neg <- train_confusion_matrix1[2, 1]/sum(train_confusion_matrix1[2, ])
  test_false_neg <- test_confusion_matrix1[2, 1]/sum(test_confusion_matrix1[2, ])
  
  train_false_neg_vector[i] <- train_false_neg
  test_false_neg_vector[i] <- test_false_neg
}

plot <- plot(1:20, train_false_neg_vector, type = "o", col = "blue", ylim = range(train_false_neg_vector,    test_false_neg_vector), xlab = "Size of tree", ylab = "false negative rates", main = paste("Data Size = 20000, Train dataset = 0.7%, CP",0))
lines(1:20, test_false_neg_vector, type = "o", col = "red")
legend("bottomright", legend = c("On training data", "On testing data"), col = c("blue", "red"), lty = 1, cex = 0.50)

dev.new()
\end{lstlisting}
\subsection{Part B}
Part B written explanation:
The cp parameter in the rpart() function is responsible for measuring the complexity and size of the decision tree. While larger cp values correspond to a simpler tree with fewer splits, smaller cp values correspond to less pruned decision trees that contain more splits. 
Part B R Code:
\begin{lstlisting}[language=R, frame=single]
#Creating 2 plots with different cp values: X axis: max depth/size of decision tree, Y axis: false negative rate

# Define a sequence of values for cp for pre-pruning
sequence_cp_pre_pruning <- c(0, 0.05)

# Generate a window for plots
par(mfrow = c(2, 2))

# Looping through each value of cp in the assigned sequence and generating a plot for each
for (j in sequence_cp_pre_pruning) {
  
  train.fnrs <- vector("numeric", 20)
  test.fnrs <- vector("numeric", 20)
  
  for (i in 1:20) {
    # Training a decision tree model using the training data 
    fit2 <- rpart(class ~ ., data = data_train1, cp = j, maxdepth = i + 1)
    predict.train2 <- predict(fit2, data_train1, type = "vector")
    predict_test2 <- predict(fit2, data_test1, type = "vector")
    
    # Calculate the false negative rates
    train_confusion_matrix2 <- table(data_train1$class, predict.train2)
    test_confusion_matrix2 <- table(data_test1$class, predict_test2)
    
    train.fnr <- train_confusion_matrix2[2, 1]/sum(train_confusion_matrix2[2, ])
    test.fnr <- test_confusion_matrix2[2, 1]/sum(test_confusion_matrix2[2, ])
    
    train.fnrs[i] <- train.fnr
    test.fnrs[i] <- test.fnr
  }
  
  plot <- plot(1:20, train.fnrs, type = "o", col = "blue", ylim = range(train.fnrs,
  test.fnrs), xlab = "Size of tree", ylab = "false negative rates", main = paste("CP=", j))
  lines(1:20, test.fnrs, type = "o", col = "red")
  legend("bottomright", legend = c("On training data", "On testing data"), col = c("blue", "red"),
         lty = 1, cex = 0.50)
}

par(mfrow = c(1, 1))

dev.new()
\end{lstlisting}
\subsection{Part C}
Part C R Code:
\begin{lstlisting}[language=R, frame=single]
#Creating 2 plots with different cp values: X axis: max depth/size of decision tree, Y axis: false negative rate

# Define a sequence of values for cp for post-pruning
sequence_cp_post_pruning <- c(0, 0.23)

# Generate a window for plots
par(mfrow = c(2, 2))

# Looping through each value of cp in the assigned sequence and generating a plot for each
for (j in sequence_cp_post_pruning) {
  
  train.fnrs1 <- vector("numeric", 20)
  test.fnrs1 <- vector("numeric", 20)
  
  for (i in 1:20) {
    # Training a decision tree model using the training data 
    fit3 <- rpart(class ~ ., data = data_train1, maxdepth = i + 1)
    
    # Implement post-pruning on the model
    fit3 <- prune(fit3, cp = j)
    predict.train3 <- predict(fit3, data_train1, type = "vector")
    predict_test3 <- predict(fit3, data_test1, type = "vector")
    
    # Calculating the false negative rates
    train_confusion_matrix3 <- table(data_train1$class, predict.train3)
    test_confusion_matrix3 <- table(data_test1$class, predict_test3)
    
    train.fnr1 <- train_confusion_matrix3[2, 1]/sum(train_confusion_matrix3[2, ])
    test.fnr1 <- test_confusion_matrix3[2, 1]/sum(test_confusion_matrix3[2, ])
    
    train.fnrs1[i] <- train.fnr1
    test.fnrs1[i] <- test.fnr1
  }
  
  plot(1:20, train.fnrs1, type = "o", col = "blue", ylim = range(train.fnrs1, test.fnrs1),
       xlab = "Size of tree", ylab = "false negative rates", main = paste("Post-pruning value=", j))
  lines(1:20, test.fnrs1, type = "o", col = "red")
  legend("bottomright", legend = c("On training data", "On testing data"), col = c("blue", "red"),
         lty = 1, cex = 0.50)
}

par(mfrow = c(1, 1))

dev.new()

\end{lstlisting}
\subsection{Part D}
Part D R Code:
\begin{lstlisting}[language=R, frame=single]
aps_failure_data_clean2 <- read.csv("C:\\Users\\surre\\Desktop\\IE 332\\A2 Q3\\aps_failure_training_set.csv", skip = 20, na.strings = "na")
aps_failure_data_clean2$class <- ifelse(aps_failure_data_clean2$class == "pos", 1, 0)

#Setting the dimension of the data set to 60000
aps_failure_data_clean2 <- head(aps_failure_data_clean2, 60000)

#splitting the clean data set into train/test sets using 80% for training and 20% for testing
create_train_test2 <- function(aps_failure_data_clean2, size = 0.80, train = TRUE) {
  n_row = nrow(aps_failure_data_clean2)
  total_row = size * n_row
  train_sample = 1: total_row
  if (train == TRUE) {
    return (aps_failure_data_clean2[train_sample, ])
  } else {
    return (aps_failure_data_clean2[-train_sample, ])
  }
}
data_train2 <- create_train_test2(aps_failure_data_clean2, 0.80, train = TRUE)
data_test2 <- create_train_test2(aps_failure_data_clean2, 0.80, train = FALSE)

#Training the decision tree using training data
fit4 <- rpart(class ~ ., data = data_train2, cp = 0, maxdepth = 5)

#Plotting decision tree
rpart.plot(fit4, type = 2)

#Applying to the testing data
predict_test4 <- predict(fit4, data_test2, type = "vector")

#Calculating testing risk
risk_test2 <- mean(predict_test4 != data_test2$class)

#Applying to training data and calculating training risk
predict.train4 <- predict(fit4, data_train2, type = "vector")
risk_train2 <- mean(predict.train4 != data_train2$class)

train.fnrs2 <- vector("numeric", 20)
test.fnrs2 <- vector("numeric", 20)

for (i in 1:20) {
  # Training a decision tree model using the training data  
  fit4 <- rpart(class ~ ., data = data_train2, cp = 0, maxdepth = i + 1)
  predict.train2 <- predict(fit4, data_train2, type = "vector")
  predict_test2 <- predict(fit4, data_test2, type = "vector")
  
  # Calculating the false negative rates
  train_confusion_matrix4 <- table(data_train2$class, predict.train2)
  test_confusion_matrix4 <- table(data_test2$class, predict_test2)
  
  train.fnr2 <- train_confusion_matrix4[2, 1]/sum(train_confusion_matrix4[2, ])
  test.fnr2 <- test_confusion_matrix4[2, 1]/sum(test_confusion_matrix4[2, ])
  
  train.fnrs2[i] <- train.fnr2
  test.fnrs2[i] <- test.fnr2
}

plot <- plot(1:20, train.fnrs2, type = "o", col = "red", ylim = range(train.fnrs2, test.fnrs2), xlab = "Size of tree", ylab = "false negative rates", main = paste("Data Size = 60000, Train dataset = 0.8%. CP = 0"))
lines(1:20, test.fnrs2, type = "o", col = "blue")
legend("bottomright", legend = c("On training data", "On testing data"), col = c("red", "blue"), lty = 1, cex = 0.50)
\end{lstlisting}
Part D Written Explanation:
Comparing the decision tree from part d to the tree from part a, we can see that the training and testing risks are lower in part d. 
 In part d, the training risk is 0.995, and the testing risk is 0.996. These are lower than the training risk of 0.996 and the testing risk of 0.997
 in part a, while still being close to each other in value.
The risk was reduced in part d by increasing the size of the data set to 60000, making the training data-set equal to 80 percent moreover, reducing the max depth of the tree from 20 to 5.
\subsection{Part E}
Part E Written Explanation:
We believe three characteristics affect the over-fitting behavior:
\begin{enumerate}
\item The size of the data set itself- the size 
of the data set should be several times bigger (about ten times according to specific rules of thumb) than the dimensionality of the data-set.
 \item The max depth/size of the tree (pre-pruning) determines when the tree will stop growing. A considerably large depth of tree can lead to over-fitting.
 \item Pruning- post-pruning can help determine the optimal cp value and can reduce over-fitting.
\end{enumerate}
\subsection{Part F}
Part F R Code:
\begin{lstlisting}[language=R, frame=single]
# Load the randomForest library
library(randomForest)
library(base)

# Using the model in Q4 and removing NA's
aps_failure_data_clean3 <- na.omit(aps_failure_data_clean2)

#Do the ramdom forest test
random_forest <- randomForest(formula = class ~ ., data = aps_failure_data_clean3) 

# Getting the importance of different variables and identifying 2 most important ones
importance <- data.frame(random_forest$importance)

# Plot the variable importance
varImpPlot(random_forest)

dev.new()

\end{lstlisting}
Part F Written Explanation:
Based on the dataset for part d, the two most important variables with the most important are "bj 000" and "am 0".
\subsection{Part G}
\begin{lstlisting}[language=R, frame=single]
install.packages("remotes", repos="http://cran.us.r-project.org")
remotes::install_github("grantmcdermott/parttree")
library(parttree)
library(parsnip)

# Create a new subset that contain class, bj_000, and am_0 (the 2 most important factors)
aps_failure_data_clean4 <- aps_failure_data_clean2[, c(1, 23, 72)]

#removing Na's
aps_failure_data_clean4 <- na.omit(aps_failure_data_clean4)

#splitting the clean data set into train/test sets using 80% for training and 20% for testing
create_train_test3 <- function(aps_failure_data_clean4, size = 0.7, train = TRUE) {
  n_row = nrow(aps_failure_data_clean4)
  total_row = size * n_row
  train_sample = 1: total_row
  if (train == TRUE) {
    return (aps_failure_data_clean4[train_sample, ])
  } else {
    return (aps_failure_data_clean4[-train_sample, ])
  }
}
data_train3 <- create_train_test3(aps_failure_data_clean4, 0.7, train = TRUE)
data_test3 <- create_train_test3(aps_failure_data_clean4, 0.7, train = FALSE)


data_train3$class = as.factor(data_train3$class)
# Building the tree by following the code from the GitHub link provided
ti_tree =
  decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification") %>%
  fit(class ~ bj_000 + am_0, data = data_train3)
ti_tree <- na.omit(ti_tree)

## Plot the data and model partitions
data_train3 %>%
  ggplot(aes(x=bj_000, y=am_0)) +
  geom_jitter(aes(col=class), alpha=0.7) +
  geom_parttree(data = ti_tree, aes(fill=class), alpha = 0.1) +
  theme_minimal()
\end{lstlisting}
\newpage

% Question 4
\fancyhead[L]{\textbf{Question 4}}
\section{Question 4}
\subsection{Part A}
Part A Written Explanation: 
Over 10 epochs for the less complex neural network comprised of 5 nodes and 1 hidden layer, there seems to be a net under-fitting when comparing the performances of the test and training results in Figure 1. This result was to be expected as under-fitting occurs when a model is too simple. This model proved to be too simple.

Part A R Code:
\begin{lstlisting}[language=R, frame=single]
library(neuralnet)
#Import the data and clean it of Na's. Convert neg,pos to 0,1.
aps_failure_data <-
  read.csv("aps_failure_training_set.csv", na.strings = c("", "na"))
aps_failure_data_clean <- na.omit(aps_failure_data)
aps_failure_data_clean$class <-
  ifelse(aps_failure_data_clean$class == "pos", 1, 0)
#Define training and testing data
training_data <- aps_failure_data_clean[1:295,]
testing_data <- aps_failure_data_clean[296:590,]
# Creates the neural net with the training data hidden = changes the complexity
#to compare performance over epochs
neural_net4a <-
  neuralnet(class ~ . - training_data$class,data = training_data,hidden = c(25,5),
    linear.output = FALSE,algorithm = "sag")
#Initialize variables to track errors
training_errors <- rep(0, 10)
testing_errors <- rep(0, 10)
#Loop over the number of epochs to collect errors
for (i in 1:10) {
#Compute errors on training data
  net_result_train <- compute(neural_net4a, training_data[])
  training_errors[i] <-sum((net_result_train$net.result[, 1] > 0.5)!= training_data[, "class"])
#Compute errors on testing data
  net_result_test <- compute(neural_net4a, testing_data[])
  testing_errors[i] <- sum((net_result_test$net.result[, 1] > 0.5)!= testing_data[, "class"])
#Update the neural network with training data
  neural_net4a <- update(neural_net4a, training_data[],training_data[, "class"])
}
#Plots the training and testing errors over the epochs
plot(training_errors,type = "l",col = "blue",xlab = "# of Epochs",ylab = "# of Errors",
  ylim = c(0, max(training_errors, testing_errors)),main = "Panel 2")
lines(testing_errors, type = "l", col = "red")
legend("bottomright",legend = c("Training Errors", "Testing Errors"),col = c("blue", "red"),lty = 1)
\end{lstlisting}

\subsection{Part B} 
Part B Written Explanation: 
As the complexity of the neural net increased, the under-fitting behavior approached a more over-fitting behavior. In Figure 3, the fit appeared closest to its 'sweet spot,' typically seen in the classical U-shaped risk curve. However, after that point, as the complexity increased in Figure 4, an over-fitting behavior began to appear, putting the test at risk. The complexity at this point was too high to predict new data accurately. The algorithm reached the point of memorization rather than learning. 

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Complexity 1.png}
    \caption{Hidden Nodes:5   Hidden Layers:1}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Complexity2.png}
    \caption{Hidden Nodes:10   Hidden Layers:1}
  \end{minipage}
\end{figure}
\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Complexity3.png}
    \caption{Hidden Nodes:10   Hidden Layers:2}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{Complexity4.png}
    \caption{Hidden Nodes:25   Hidden Layers:2}
  \end{minipage}
\end{figure}


\subsection{Part C} 
Part C Written Explanation:
The results from part B in this question and part E in question 3 align. The final takeaway from Q3E was that pruning/post-pruning can optimize the tree to reduce over-fitting. A reduction in depth (or complexity) can prevent this from occurring in both parts.

\newpage
% Question 5
\fancyhead[L]{\textbf{Question 5}}
\section{Question 5}
Libraries used for Question 5:
\begin{lstlisting}[language=R, frame=single]
library(tidyverse)
library(cluster)
library(factoextra)
library(gridExtra)
library(dplyr)
library(caret)
library(car)
library(glmnet)
\end{lstlisting}
\subsection{Part A} 
Part A R Code:
\begin{lstlisting}[language=R, frame=single]
#Import Data. Ensure this .r file is in the same folder/working directory as the .csv
real_estate_data_raw <- read.csv("train.csv", header=TRUE)


#Clean data. Remove NAs, scale, and select only numerical columns. Remove Zero variance columns
real_estate_data <-select_if(real_estate_data_raw, is.numeric)
re_data_frame<- na.omit(real_estate_data)
re_data_frame<- re_data_frame[sample(nrow(re_data_frame),50),] #  <----- THIS IS AMOUNT OF PROPERTIES TO BE INCLUDED IN CLUSTERING
Var <- apply(re_data_frame, 2, var)
zero_var_col <- which (Var == 0)
re_data_frame<- re_data_frame[, -zero_var_col]

#Create Cluster.
cluster3 <- kmeans(re_data_frame, centers = 4, nstart = 5)
#Collect Individual Cluster Data
cluster1_data <- re_data_frame[cluster3$cluster == 1, ]
cluster2_data <- re_data_frame[cluster3$cluster == 2, ]
cluster3_data <- re_data_frame[cluster3$cluster == 3, ]
cluster4_data <- re_data_frame[cluster3$cluster == 4, ]
c1qual <- mean(cluster1_data$OverallQual)
c2qual <- mean(cluster2_data$OverallQual)
c3qual <- mean(cluster3_data$OverallQual)
c4qual <- mean(cluster4_data$OverallQual)

#Plots the Clusters. Prints Average Quality of cluster
plot3 <-fviz_cluster(cluster3, data = re_data_frame)
grid.arrange(plot3)
cat("The Average Overall Quality Rating of Cluster 1 is", sprintf("%.2f", c1qual))
cat("The Average Overall Quality Rating of Cluster 2 is", sprintf("%.2f", c2qual))
cat("The Average Overall Quality Rating of Cluster 3 is", sprintf("%.2f", c3qual))
cat("The Average Overall Quality Rating of Cluster 4 is", sprintf("%.2f", c4qual))

\end{lstlisting}
\subsection{Part B} 
Part B Written Explanation:
After the clustering algorithm executes, the data of all 4 clusters are collected. The script then analyzes this data to provide printing of the average overall quality of each cluster's properties. This is a strong predictor of an increase in real-estate prices. A prospective buyer is looking for an investment, and a piece of real estate with excellent overall quality would be the buyer's target. The clusters are created from a random selection of 50 properties from the overall data set so that the cluster remains readable but contains enough data to be helpful. Whichever cluster has the highest overall quality is likely to increase in price. 
\subsection{Part C} 
Part C Written Explanation:
The regression machine learning algorithm used for all numerical real-estate data saw an r-squared value of 0.765. This value shows a high correlation between the predicted values and the actual values in the testing data set used for the algorithm. The group regards this as a positive result regarding the performance of our linear regression model. The model we used predicted the overall quality of the properties.

Part C R Code:
\begin{lstlisting}[language=R, frame=single]
#Regression machine learning algorithm on all data for overall quality of
#property 5C--------------------------------------------------------------------
#Creating Training Data
training <- real_estate_data[1:730,]
training <- na.omit(training)
training <- training[1:550,]

#Creating Testing Data
testing <- real_estate_data[731:1460,]
testing <- na.omit(testing)
testing <- testing[1:550,]

#Fitting Model to predict overall quality
reg_model <- lm(training$OverallQual~.-training$Id -training$MSSubClass,data = training)

#Testing Model ability to predict overall quality
predictions <- predict(reg_model, newdata = testing)
Rsquare <- sqrt(mean((testing$OverallQual - predictions)^2))
print(paste0("R-squared value is: ", round(Rsquare, 3)))
\end{lstlisting}

\subsection{Part D} 
Part D Written Explanation:
The regression machine learning algorithm used for the individual clusters of real-estate data was an altered version of the algorithm used in part 5c. Instead of using all the data, the algorithm collected the data from each cluster in a data frame as the clusters were constructed. The group wanted to test and train with an equal amount of rows of data, so if a cluster had an odd amount of data entities, one was omitted to allow for an even, whole-numbered split. The clusters saw anywhere between 0.69 - 0.78 r-squared values when the algorithm was run, and clusters were created. These results are seen as positive. However, the group made notice of an anomaly that occurred when the script was executed. One of the clusters had an r-squared value of 32. As R-squared values should be between 0-1, this is an inaccurate prediction for that specific cluster. Despite debugging efforts, the group could not reach the issue's root.

Part D R Code:
\begin{lstlisting}[language=R, frame=single]
#Regression machine learning algorithm on individual cluster data for overall 
#quality of property 5D------------------------------------------------------------

#Collect Individual Cluster Data
cluster1_data <- re_data_frame[cluster_alg$cluster == 1, ]
cluster2_data <- re_data_frame[cluster_alg$cluster == 2, ]
cluster3_data <- re_data_frame[cluster_alg$cluster == 3, ]
cluster4_data <- re_data_frame[cluster_alg$cluster == 4, ]

#Determine size of Test and Train data frames for each cluster. If even, split. If odd ommit 1 and split
cluster11_counts <- nrow(cluster1_data)
if (cluster11_counts %% 2 == 1) {
  cluster12_counts <- floor((cluster11_counts - 1) / 2)
} else {
  cluster12_counts <- cluster11_counts / 2
}

cluster21_counts <- nrow(cluster2_data)
if (cluster21_counts %% 2 == 1) {
  cluster22_counts <- floor((cluster21_counts - 1) / 2)
} else {
  cluster22_counts <- cluster21_counts / 2
}

cluster31_counts <- nrow(cluster3_data)
if (cluster31_counts %% 2 == 1) {
  cluster32_counts <- floor((cluster31_counts - 1) / 2)
} else {
  cluster32_counts <- cluster31_counts / 2
}

cluster41_counts <- nrow(cluster4_data)
if (cluster41_counts %% 2 == 1) {
  cluster42_counts <- floor((cluster41_counts - 1) / 2)
} else {
  cluster42_counts <- cluster41_counts / 2
}

#Creating Training and Testing Data Data Cluster 1,2,3,4
training_c1 <- cluster1_data[1:cluster12_counts,]
testing_c1 <- cluster1_data[(cluster12_counts+1):cluster11_counts,]
training_c2 <- cluster2_data[1:cluster22_counts,]
testing_c2 <- cluster2_data[(cluster22_counts+1):cluster21_counts,]
training_c3 <- cluster3_data[1:cluster32_counts,]
testing_c3 <- cluster3_data[(cluster32_counts+1):cluster31_counts,]
training_c4 <- cluster4_data[1:cluster42_counts,]
testing_c4 <- cluster4_data[(cluster42_counts+1):cluster41_counts,]

#Fitting each cluster a regression model to predict overall quality
reg_model_c1 <- lm(training_c1$OverallQual~.,data = training_c1)
reg_model_c2 <- lm(training_c2$OverallQual~.,data = training_c2)
reg_model_c3 <- lm(training_c3$OverallQual~.,data = training_c3)
reg_model_c4 <- lm(training_c4$OverallQual~.,data = training_c4)

#Test the Models ability to predict overall quality
predictions_c1 <- predict(reg_model_c1, newdata = testing_c1)
Rsquare_c1 <- sqrt(mean((testing_c1$OverallQual - predictions_c1)^2))
print(paste0("R-squared value for the cluster 1 model is: ", round(Rsquare_c1, 3)))
predictions_c2 <- predict(reg_model_c2, newdata = testing_c2)
Rsquare_c2 <- sqrt(mean((testing_c2$OverallQual - predictions_c2)^2))
print(paste0("R-squared value for the cluster 2 model is: ", round(Rsquare_c2, 3)))
predictions_c3 <- predict(reg_model_c3, newdata = testing_c3)
Rsquare_c3 <- sqrt(mean((testing_c3$OverallQual - predictions_c3)^2))
print(paste0("R-squared value for the cluster 3 model is: ", round(Rsquare_c3, 3)))
predictions_c4 <- predict(reg_model_c4, newdata = testing_c4)
Rsquare_c4 <- sqrt(mean((testing_c4$OverallQual - predictions_c4)^2))
print(paste0("R-squared value for the cluster 4 model is: ", round(Rsquare_c4, 3)))
\end{lstlisting}
\end{document}
